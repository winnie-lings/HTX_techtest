{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7689c6d-e428-49c7-a822-e73d4e00bfbf",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdaf46-e07b-4b97-b1ba-dd3cf8a43ab3",
   "metadata": {},
   "source": [
    "### Task 3a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9ab6a2-4e21-4925-87ff-d172de792122",
   "metadata": {},
   "source": [
    "#### 1. Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb6889c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, EvalPrediction,  TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import jiwer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5f0e0b-852c-4452-ba31-347335bde2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using the CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. You can use the GPU.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using the CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726ef5a-7c21-450e-9a5a-6cb626cbd733",
   "metadata": {},
   "source": [
    "#### 2. Initialize the Wav2Vec2Processor and model\n",
    "- Wav2Vec2Processor from the Hugging Face transformers library to handle both audio feature extraction and text tokenization.\n",
    "  - Feature Extraction for audio (converting raw audio into a format compatible with Wav2Vec2)\n",
    "  - Text Tokenization for transcriptions (converting transcriptions into token IDs that the model can process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c009fdc5-936d-4b0f-aa83-01e999a1cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Wav2Vec2 processor \n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29ce72-8182-4b1b-bf9b-bd1e027ef1a5",
   "metadata": {},
   "source": [
    "#### 3. Audio Preprocessing with Wav2Vec2Processor\n",
    "- Audio was resampled to 16kHz as wav2vec2 model was pretrained on 16kHz sampled speech audio (target sample rate)\n",
    "- Audio was normalised to ensure consistency across different files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484a2e0-3bda-4152-8594-e86b1d3776c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, target_sr=16000, max_length=16000):\n",
    "    \n",
    "    # Load audio file with Librosa (default sampling rate is None, so it loads as is)\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "\n",
    "    # Resample if the sampling rate is different\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Normalize the audio to [-1, 1]\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    # Check for NaNs or Infs\n",
    "    if np.any(np.isnan(audio)) or np.any(np.isinf(audio)):\n",
    "        print(f\"Warning: NaN or Inf detected in audio file {file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Truncate or pad the audio to the fixed length\n",
    "    if len(audio) > max_length:\n",
    "        audio = audio[:max_length]  # Truncate to max_length\n",
    "    elif len(audio) < max_length:\n",
    "        padding = max_length - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')  # Pad with zeros\n",
    "\n",
    "    # Convert audio to numpy array and process with Wav2Vec2 feature extractor\n",
    "    input_values = processor(audio, sampling_rate=target_sr, return_tensors=\"pt\").input_values  # Use processor to get input values\n",
    "\n",
    "    # Ensure the input has the correct shape [batch_size, num_channels, num_frames]\n",
    "    # The model expects input shape: [batch_size, num_channels, num_frames]\n",
    "    # Here, batch_size = 1, num_channels = 1 (mono audio), num_frames = length of the audio signal\n",
    "    input_values = input_values.squeeze(0)  # Remove the batch dimension to get [num_channels, num_frames]\n",
    "    print(f\"Input shape after squeeze: {input_values.shape}\")  # Check the shape\n",
    "    \n",
    "    input_values = input_values.unsqueeze(0)  # Add the batch dimension back\n",
    "    # Make sure the shape is [1, num_frames] (num_channels = 1)\n",
    "    print(f\"Input shape after unsqueeze: {input_values.shape}\")  # Check the shape again\n",
    "    \n",
    "    # Move the input values to the same device as the model (GPU or CPU)\n",
    "    input_values = input_values.to(device)\n",
    "    \n",
    "    print(input_values.shape)\n",
    "    \n",
    "    return input_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524ab9d-7a88-40a9-862c-aceccdfdb19f",
   "metadata": {},
   "source": [
    "#### 4. Text Tokenization with Wav2Vec2Processor\n",
    "- Converting transcription to input IDs (numerical representation of the transcription)\n",
    "- Padding/truncating the text to a fixed maximum length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "076ed850-67ea-40ed-8c6c-9ba4d9b63c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_transcription(transcription, processor, max_length=256):\n",
    "    # Tokenize transcription using Wav2Vec2Processor\n",
    "    tokenized = processor(text=transcription, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return tokenized.input_ids.squeeze(0)\n",
    "    #torch.tensor(tokenized['input_ids'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae4bce-da16-467f-b6a1-9210a2c9c10d",
   "metadata": {},
   "source": [
    "#### 5. Custom Dataset class to load and process the audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66271da-8760-49f4-b8f3-1f4774d4ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonVoiceDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.audio_files = dataframe['filename'].values\n",
    "        self.transcriptions = dataframe['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess the audio\n",
    "        input_values = preprocess_audio(self.audio_files[idx])\n",
    "\n",
    "        # Tokenize transcription\n",
    "        transcription = self.transcriptions[idx]\n",
    "        labels = tokenize_transcription(transcription, self.processor)\n",
    "        \n",
    "        return {\n",
    "            'input_values': input_values,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf128ae-9e4c-4b28-a20f-a33c8a2077f6",
   "metadata": {},
   "source": [
    "#### 6. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97a70e06-e9e2-43fa-9e8b-a306d5d577c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...\n",
      "1    C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...\n",
      "2    C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...\n",
      "3    C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...\n",
      "4    C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...\n",
      "Name: filename, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder of the downloaded data \"Common Voice\"\n",
    "cv_folder = 'C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techtest/common_voice/'\n",
    "\n",
    "# Path to the folder that contains the common-voice mp3 files under cv-valid-train folder\n",
    "audio_folder = os.path.join(cv_folder, 'cv-valid-train/')\n",
    "\n",
    "# Path to the CSV file that contains the metadata\n",
    "csv_file = os.path.join(cv_folder, 'cv-valid-train.csv')\n",
    "\n",
    "cv_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Update the file paths in the Dataframe\n",
    "cv_df['filename'] = cv_df['filename'].apply(lambda x: os.path.join(audio_folder, x))\n",
    "\n",
    "# Check the first few paths\n",
    "print(cv_df['filename'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e734b1f2-8c1a-4b3e-be65-6114b8250d40",
   "metadata": {},
   "source": [
    "#### 7. Training and Validation Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a845e84-b65b-421b-bb92-b6259a177474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 137043\n",
      "Validation set size: 58733\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into 70% training and 30% validation\n",
    "train_df, val_df = train_test_split(cv_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Check the split\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c1333-3a88-474a-bb0d-c47b213b5556",
   "metadata": {},
   "source": [
    "#### 8. Create DataLoaders for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937dbb7-4327-48dc-8ee0-d5b44baf1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for training and validation\n",
    "train_dataset = CommonVoiceDataset(train_df, processor)\n",
    "val_dataset = CommonVoiceDataset(val_df, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81951da3-cebe-43a3-9d78-10fe6e9cb1b8",
   "metadata": {},
   "source": [
    "#### 9. Fine-tuning the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83a8049f-9472-44d8-abb0-01bba4ee314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word error rate (WER) metric using the evaluate library for evaluation\n",
    "wer_metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864243a7-d5cf-4ae4-a5d3-d5419032c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_wer(prediction: EvalPrediction):\n",
    "    # Convert model output logits to predicted text\n",
    "    pred_ids = prediction.predictions.argmax(axis=-1)  # Choose the predicted token\n",
    "    pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Decode the references (ground truth transcriptions)\n",
    "    labels = prediction.label_ids\n",
    "    ref_text = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute WER\n",
    "    return wer_metric.compute(predictions=pred_text, references=ref_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a2699-b946-418a-81c0-6452158c51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781045d9-6eb1-4624-8083-e87dc347f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the pre-trained Wav2Vec2 model for CTC (Connectionist Temporal Classification)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\", ctc_loss_reduction=\"mean\", pad_token_id=processor.tokenizer.pad_token_id)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-finetuned\",  # Where the model outputs will be saved\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n",
    "    learning_rate=1e-3,  # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,  # Batch size for evaluation\n",
    "    num_train_epochs=2,  # Number of training epochs. Best to include more epochs so model can learn better but was limited by computational resources\n",
    "    logging_dir=\"./logs\",  # Where logs will be saved\n",
    "    logging_steps=500,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over smaller batches\n",
    "    lr_scheduler_type=\"cosine\",  # Use cosine annealing for learning rate decay\n",
    "    save_steps=500,  # Save model every 500 steps\n",
    "    weight_decay=0.01,  # Weight decay for regularization,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=True  # makes training more efficient by grouping training samples of similar input length into one batch. \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=lambda data: {\n",
    "        'input_values': torch.stack([item['input_values'] for item in data]),\n",
    "        'labels': torch.stack([item['labels'] for item in data])    \n",
    "    },\n",
    "    tokenizer=processor.feature_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e40154-d4f6-4ea8-8797-34bc48ab5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af434f-8513-4aea-bccf-57f49101a03b",
   "metadata": {},
   "source": [
    "#### I am sorry I did not manage to debug and fine-tune the model. I was limited by the GPU. I also ran the code in Google Colab but was limited by the resources available too. Hence, for subsequent tasks (3c, 4, 5) which require the use of the fine-tuned model or the transcribed results from it, I have used the base model (\"facebook/wav2vec2-base-960h\") as the \"fine-tuned\" model so I can continue with the tasks. My apologies for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d381b0-8a2b-435d-b302-f758d2a28d8c",
   "metadata": {},
   "source": [
    "#### 10.Evaluate and visualize Metrics\n",
    "Visualize the training and validation loss to check how well the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce70ab9-767d-41d3-a969-1a3ab29142d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# Visualize the training and validation loss\n",
    "train_loss = [x['loss'] for x in trainer.state.log_history if 'loss' in x]\n",
    "eval_loss = [x['eval_loss'] for x in trainer.state.log_history if 'eval_loss' in x]\n",
    "\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(eval_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2d199-7391-40bb-94e7-0230ae016e0e",
   "metadata": {},
   "source": [
    "### Task 3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534ffd2-206c-4c8d-b6b7-e771d7438043",
   "metadata": {},
   "source": [
    "#### 11. Save the fine-tuned model\n",
    "Once the model has finished training, save the fine-tuned model and the processor for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2f3c53-f2c3-4ce6-b825-1d4ebde3eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./wav2vec2-large-960h-cv-finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2-large-960h-cv-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f2f0d-19e5-437e-b40e-c681da7fc487",
   "metadata": {},
   "source": [
    "### Task 3c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d1683-0dca-4a56-9fcd-24558d70e5f5",
   "metadata": {},
   "source": [
    "#### Do note that for subsequent tasks, I have used the base model (\"facebook/wav2vec2-base-960h\") for the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db0612c-dd08-404b-a3cc-bb738c39e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f592cc1d-f637-4983-81b4-896fb7634ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>duration</th>\n",
       "      <th>generated_text_fine-tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...</td>\n",
       "      <td>without the dataset the article is useless</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>without the dat asset the articles useless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...</td>\n",
       "      <td>i've got to go to him</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ive gat go to him</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...</td>\n",
       "      <td>and you know it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and you know it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...</td>\n",
       "      <td>down below in the darkness were hundreds of pe...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>down below in the darkness were hundreds of pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...</td>\n",
       "      <td>hold your nose to keep the smell from disablin...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hold your nose to keep the smell from disablin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
       "1  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
       "2  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
       "3  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
       "4  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
       "\n",
       "                                                text  up_votes  down_votes  \\\n",
       "0         without the dataset the article is useless         1           0   \n",
       "1                              i've got to go to him         1           0   \n",
       "2                                    and you know it         1           0   \n",
       "3  down below in the darkness were hundreds of pe...         4           0   \n",
       "4  hold your nose to keep the smell from disablin...         2           0   \n",
       "\n",
       "        age gender accent  duration  \\\n",
       "0       NaN    NaN    NaN       NaN   \n",
       "1  twenties   male    NaN       NaN   \n",
       "2       NaN    NaN    NaN       NaN   \n",
       "3  twenties   male     us       NaN   \n",
       "4       NaN    NaN    NaN       NaN   \n",
       "\n",
       "                           generated_text_fine-tuned  \n",
       "0         without the dat asset the articles useless  \n",
       "1                                  ive gat go to him  \n",
       "2                                    and you know it  \n",
       "3  down below in the darkness were hundreds of pe...  \n",
       "4  hold your nose to keep the smell from disablin...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the folder that contains the mp3 files and the CSV file\n",
    "cv_folder = 'C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techtest/common_voice/'  # Update this to your local path\n",
    "audio_folder = os.path.join(cv_folder, 'cv-valid-test/')\n",
    "csv_file = os.path.join(cv_folder, 'cv-valid-test.csv')\n",
    "\n",
    "# Load the CSV file into a dataframe\n",
    "cv_df = pd.read_csv(csv_file)\n",
    "\n",
    "# Update the 'filename' column to include the full path to the audio files\n",
    "cv_df['filename'] = cv_df['filename'].apply(lambda x: os.path.join(audio_folder, x))\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16354a49-880b-4a04-9645-b0306c156399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(file_path, target_sr=16000):\n",
    "    # Load audio file using librosa\n",
    "    audio, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Resample audio if needed\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Normalize the audio to [-1, 1]\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    \n",
    "    # Convert to tensor for Wav2Vec2 processor\n",
    "    input_values = processor(audio, sampling_rate=target_sr, return_tensors=\"pt\").input_values\n",
    "    input_values = input_values.squeeze(0)\n",
    "    input_values = input_values.unsqueeze(0)\n",
    "    return input_values  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f41529-6ae0-48f5-aa7e-6351ee257ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    # Preprocess the audio file\n",
    "    input_values = preprocess_audio(file_path)\n",
    "    \n",
    "    # Move input values to the same device as the model\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    # Perform inference (model outputs logits)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    \n",
    "    # Use argmax to get the most probable token ids\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Decode the token ids to text\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    \n",
    "    return transcription[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573321df-630d-4b49-9705-f95b3c0de6c4",
   "metadata": {},
   "source": [
    "##### Generated text stored in cv-valid-test.csv under column \"generated_text_fine-tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e7a4eb-9bf3-45d4-bd06-bc97bfd80392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  \\\n",
      "0  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
      "1  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
      "2  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
      "3  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
      "4  C:/Users/lingy/OneDrive/Desktop/HTX/HTX_techte...   \n",
      "\n",
      "                           generated_text_fine-tuned  \n",
      "0         without the dat asset the articles useless  \n",
      "1                                  ive gat go to him  \n",
      "2                                    and you know it  \n",
      "3  down below in the darkness were hundreds of pe...  \n",
      "4  hold your nose to keep the smell from disablin...  \n"
     ]
    }
   ],
   "source": [
    "# Create a new column with transcriptions\n",
    "cv_df['generated_text_fine-tuned'] = cv_df['filename'].apply(lambda x: transcribe_audio(x))\n",
    "cv_df['generated_text_fine-tuned'] = cv_df['generated_text_fine-tuned'].str.lower()\n",
    "\n",
    "# Print a sample to verify\n",
    "print(cv_df[['filename', 'generated_text_fine-tuned']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78173fc0-f5e8-421e-8e9e-8bca4cae501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_generated_text_fine_tuned(df):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        # Get the file path and ground truth transcription\n",
    "        predicted_text = row['generated_text_fine-tuned']\n",
    "        ground_truth = row['text']\n",
    "        \n",
    "        # Append predictions and references for evaluation\n",
    "        predictions.append(predicted_text)\n",
    "        references.append(ground_truth)\n",
    "    \n",
    "    # Compute the Word Error Rate (WER)\n",
    "    wer_score = wer_metric.compute(predictions=predictions, references=references)\n",
    "    return wer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8672ac-d7d5-4fa2-b802-2a313ee0447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3995/3995 [00:00<00:00, 4565.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Error Rate (WER) on the cv_valid_dev set (groundtruth VS fine-tuned): 13.67%\n"
     ]
    }
   ],
   "source": [
    "wer_score = evaluate_model_generated_text_fine_tuned(cv_df)\n",
    "\n",
    "# Log the overall performance\n",
    "print(f\"Word Error Rate (WER) on the cv_valid_dev set (groundtruth VS fine-tuned): {wer_score * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccf612-dfa8-4bc5-8abd-b3636d288e20",
   "metadata": {},
   "source": [
    "#### Word Error Rate (WER) of \"fine-tuned\" model on the cv-valid-test set: 13.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6f4ac-3112-43be-a345-ac8d16965540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
